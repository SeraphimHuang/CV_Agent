from __future__ import annotations

"""
Google Geminiå®¢æˆ·ç«¯æ–°å®ç°
ä½¿ç”¨ `google.genai.Client` æŒ‰å®˜æ–¹ç¤ºä¾‹è°ƒç”¨ Gemini APIã€‚
"""

import asyncio
from google import genai

from config.prompt_manager import PromptManager
from llm.base_client import BaseLLMClient


class GeminiClient(BaseLLMClient):
    """Google Geminiå®¢æˆ·ç«¯ï¼ˆåŸºäº genai.Clientï¼‰"""

    def __init__(self, api_key: str, prompt_manager: PromptManager):
        super().__init__(prompt_manager, "gemini")
        # ä½¿ç”¨ç¤ºä¾‹ä¸­çš„å®˜æ–¹ Clientï¼Œç›´æ¥ä¼ å…¥ API key
        self.client = genai.Client(api_key=api_key)
        self._use_client = True

    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ Gemini API å¹¶è¿”å›æ–‡æœ¬"""
        print("ğŸŸ¡ Gemini API è°ƒç”¨å¼€å§‹...")
        response = await asyncio.to_thread(
            self.client.models.generate_content,
            model=self.config["model"],  # "gemini-2.5-flash"
            contents=prompt,
        )
        result_text = response.text
        print("ğŸŸ¡ Gemini API è°ƒç”¨å®Œæˆ")
        return result_text 