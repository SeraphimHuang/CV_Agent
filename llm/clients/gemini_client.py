"""
Google Geminiå®¢æˆ·ç«¯å®ç°
"""

import asyncio
import google.generativeai as genai
from config.prompt_manager import PromptManager
from llm.base_client import BaseLLMClient


class GeminiClient(BaseLLMClient):
    """Google Geminiå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, prompt_manager: PromptManager):
        super().__init__(prompt_manager, 'gemini')
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(self.config['model'])
    
    async def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨Gemini API"""
        print(f"ğŸŸ¡ Gemini API è°ƒç”¨å¼€å§‹...")
        generation_config = {
            "temperature": self.config['temperature'],
            "max_output_tokens": self.config['max_output_tokens'],
        }
        
        response = await asyncio.to_thread(
            self.model.generate_content,
            prompt,
            generation_config=generation_config
        )
        print(f"ğŸŸ¡ Gemini API è°ƒç”¨å®Œæˆ")
        return response.text 